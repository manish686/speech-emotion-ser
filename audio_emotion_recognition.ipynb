{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN17oS8NfX4L"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d uldisvalainis/audio-emotions\n",
        "!unzip audio-emotions.zip -d audio_emotions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dividing into test train\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "root = \"/content/audio_emotions/Emotions\"  # change to your path\n",
        "\n",
        "rows = []\n",
        "for emotion in sorted(os.listdir(root)):\n",
        "    emotion_dir = os.path.join(root, emotion)\n",
        "    if not os.path.isdir(emotion_dir):\n",
        "        continue\n",
        "    for fname in os.listdir(emotion_dir):\n",
        "        if not fname.lower().endswith((\".wav\", \".flac\", \".mp3\")):\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"filepath\": os.path.join(emotion_dir, fname),\n",
        "            \"label\": emotion\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"all_audio_with_labels.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "RbCqwvS-k5v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80% train, 20% temp (val+test)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
        ")\n",
        "\n",
        "# split temp into 50/50 -> 10% val, 10% test overall\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42\n",
        ")\n",
        "\n",
        "train_df.to_csv(\"train.csv\", index=False)\n",
        "val_df.to_csv(\"val.csv\", index=False)\n",
        "test_df.to_csv(\"test.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "l77gUISXlyWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers datasets soundfile librosa"
      ],
      "metadata": {
        "id": "Ep1himPGLb8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# 1. Config\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"superb/hubert-large-superb-er\"   # pretrained SER model\n",
        "TARGET_SR = 16000\n",
        "\n",
        "# 2. Load model + feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForAudioClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# 3. Load test split\n",
        "test_df = pd.read_csv(\"test.csv\")   # must contain columns: filepath, label\n",
        "\n",
        "label2idx = {lab: i for i, lab in enumerate(sorted(test_df[\"label\"].unique()))}\n",
        "idx2label = {i: lab for lab, i in label2idx.items()}\n",
        "y_true = np.array([label2idx[l] for l in test_df[\"label\"]])\n",
        "\n",
        "# 4. Inference helper\n",
        "def predict_file(path):\n",
        "    # Load mono audio at 16 kHz\n",
        "    speech, sr = librosa.load(path, sr=TARGET_SR)\n",
        "\n",
        "    # No truncation flag here; let the extractor handle it\n",
        "    inputs = feature_extractor(\n",
        "        speech,\n",
        "        sampling_rate=TARGET_SR,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        # truncation=False  # default; do not set truncation=True without max_length\n",
        "    )\n",
        "    input_values = inputs[\"input_values\"].to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_values)\n",
        "        logits = outputs.logits\n",
        "    pred_idx = int(logits.argmax(dim=-1).cpu().item())\n",
        "    return pred_idx\n",
        "\n",
        "# 5. Run zero-shot evaluation\n",
        "y_pred = []\n",
        "for path in test_df[\"filepath\"]:\n",
        "    y_pred.append(predict_file(path))\n",
        "\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "    y_true, y_pred, average=\"weighted\", zero_division=0\n",
        ")\n",
        "\n",
        "print(\"Zero-shot evaluation (pretrained model, no fine-tuning)\")\n",
        "print(f\"Accuracy : {acc:.3f}\")\n",
        "print(f\"Precision: {prec:.3f}\")\n",
        "print(f\"Recall   : {rec:.3f}\")\n",
        "print(f\"F1-score : {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "YT3ERSOYmBjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. train/fine tune AM model"
      ],
      "metadata": {
        "id": "_im9EeRALTf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers soundfile librosa scikit-learn --quiet\n"
      ],
      "metadata": {
        "id": "bLR2AT0WOLkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"superb/hubert-large-superb-er\"\n",
        "TARGET_SR = 16000\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 10\n",
        "LR = 1e-5\n"
      ],
      "metadata": {
        "id": "L_SS--NcOOWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "val_df   = pd.read_csv(\"val.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")\n",
        "\n",
        "all_labels = sorted(pd.concat([train_df[\"label\"], val_df[\"label\"], test_df[\"label\"]]).unique())\n",
        "label2idx = {lab: i for i, lab in enumerate(all_labels)}\n",
        "idx2label = {i: lab for lab, i in label2idx.items()}\n",
        "\n",
        "train_df[\"y\"] = train_df[\"label\"].map(label2idx)\n",
        "val_df[\"y\"]   = val_df[\"label\"].map(label2idx)\n",
        "test_df[\"y\"]  = test_df[\"label\"].map(label2idx)\n"
      ],
      "metadata": {
        "id": "2qtBmTvGOOZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(all_labels),        # adapt head to your label count\n",
        "    ignore_mismatched_sizes=True,      # replace old classification layer\n",
        ").to(DEVICE)\n"
      ],
      "metadata": {
        "id": "UdCzRyWiOOcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SERDataset(Dataset):\n",
        "    def __init__(self, df, sr=TARGET_SR):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.sr = sr\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        wav_path = row[\"filepath\"]\n",
        "        label = int(row[\"y\"])\n",
        "        speech, sr = librosa.load(wav_path, sr=self.sr)\n",
        "        return speech, label\n",
        "\n",
        "def collate_fn(batch):\n",
        "    speeches = [b[0] for b in batch]\n",
        "    labels   = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
        "    inputs = feature_extractor(\n",
        "        speeches,\n",
        "        sampling_rate=TARGET_SR,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    )\n",
        "    input_values = inputs[\"input_values\"]\n",
        "    return input_values, labels\n",
        "\n",
        "train_ds = SERDataset(train_df)\n",
        "val_ds   = SERDataset(val_df)\n",
        "test_ds  = SERDataset(test_df)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "-aVjaFq2OOfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Force CUDA if available\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"CUDA is not available, cannot force GPU.\")\n",
        "DEVICE = torch.device(\"cuda\")  # or torch.device(\"cuda:0\")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        # move batch to GPU\n",
        "        inputs = inputs.to(DEVICE, non_blocking=True)\n",
        "        labels = labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            outputs = model(inputs)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
        "    val_loss, val_acc = run_epoch(val_loader, train=False)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_finetuned_ser.pth\")\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train_loss={train_loss:.3f} acc={train_acc:.3f} | \"\n",
        "        f\"val_loss={val_loss:.3f} acc={val_acc:.3f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "0ncIkU4MOOhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_finetuned_ser.pth\", map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "all_true, all_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        all_true.extend(labels.cpu().numpy())\n",
        "        all_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "all_true = np.array(all_true)\n",
        "all_pred = np.array(all_pred)\n",
        "\n",
        "acc = accuracy_score(all_true, all_pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "    all_true, all_pred, average=\"weighted\", zero_division=0\n",
        ")\n",
        "\n",
        "print(\"Fine-tuned model evaluation on test set\")\n",
        "print(f\"Accuracy : {acc:.3f}\")\n",
        "print(f\"Precision: {prec:.3f}\")\n",
        "print(f\"Recall   : {rec:.3f}\")\n",
        "print(f\"F1-score : {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "-V4BCF7xOOkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrCIfvoOOOm3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}